<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: nlp | Alejandro Nolla - z0mbiehunt3r]]></title>
  <link href="http://blog.alejandronolla.com/blog/categories/nlp/atom.xml" rel="self"/>
  <link href="http://blog.alejandronolla.com/"/>
  <updated>2013-05-29T14:10:45+02:00</updated>
  <id>http://blog.alejandronolla.com/</id>
  <author>
    <name><![CDATA[Alejandro Nolla]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[N-Gram-Based Text Categorization: Categorizing text with python]]></title>
    <link href="http://blog.alejandronolla.com/2013/05/20/n-gram-based-text-categorization-categorizing-text-with-python/"/>
    <updated>2013-05-20T20:06:00+02:00</updated>
    <id>http://blog.alejandronolla.com/2013/05/20/n-gram-based-text-categorization-categorizing-text-with-python</id>
    <content type="html"><![CDATA[<h2>Introduction</h2>

<p>As we saw in last post it's really easy to detect text language using an analysis of stopwords. Another way to detect language, or when syntax rules are not being followed, is using N-Gram-Based text categorization (useful also for identifying the topic of the text and not just language) as William B. Cavnar and John M. Trenkle <a href="/images/upload/2013/05/10.1.1.53.9367.pdf">wrote</a> in 1994 so i decided to mess around a bit and did <a href="https://github.com/z0mbiehunt3r/ngrambased-textcategorizer">ngrambased-textcategorizer</a> in python as a proof of concept. <!-- more --></p>

<h2>How N-Gram-Based Text Categorization works</h2>

<p>To perform N-Gram-Based Text Categorization we need to compute N-grams (with N=1 to 5) for each word - and apostrophes - found in the text, doing something like (being the word "TEXT"):</p>

<ul>
<li>bi-grams: _T, TE, EX, XT, T_</li>
<li>tri-grams: _TE, TEX, EXT, XT_, T_ _</li>
<li>quad-grams: _TEX, TEXT, EXT_, XT_ _, T_ _ _</li>
</ul>


<p>When every N-Gram has been computed we just keep top 300 - William and John observed this range as proper for language detection and starting around 300 for subject categorization - and save them as a "text category profile". For categorize a text we only have to make same steps and calculate the "Out-Of-Place" measure against pre-computed profiles:<br/>
<img src="/images/upload/2013/05/outofplace_measure.png"></p>

<p>Then, choose the nearest one - the one with lower distance - among them.</p>

<h2>Implementing it in python</h2>

<p>Procedure to create a text category profile is well explained at point "3.1 Generating N-Gram Frequency Profiles" and it's really easy to implement it in python with the help of powerful <a href="http://nltk.org/index.html">nltk</a> toolkit.</p>

<h3>First step: Split text into tokens (tokenization)</h3>

<p>We need to tokenize text splitting by strings of only letters and apostrophes so we could use nltk <a href="http://nltk.org/api/nltk.tokenize.html?highlight=regexp#module-nltk.tokenize.regexp">RegexpTokenizer</a> for this:
``` python tokenizing by regular expression</p>

<blockquote><blockquote><blockquote><p>from nltk.tokenize import RegexpTokenizer
tokenizer = RegexpTokenizer("[a-zA-Z'`éèî]+")
tokenizer.tokenize("Le temps est un grand maître, dit-on, le malheur est qu'il tue ses élèves.")
['Le', 'temps', 'est', 'un', 'grand', 'ma\xc3\xaetre', 'dit', 'on', 'le', 'malheur', 'est', "qu'il", 'tue', 'ses', '\xc3\xa9l\xc3\xa8ves']
```</p></blockquote></blockquote></blockquote>

<p>It's just a proof of concept and should be tuned, but will be enough for now.</p>

<h3>Second step: Generating N-grams for each token</h3>

<p>Now it's time to generate n-grams (with N=1 to 5) using blank as padding, again nltk has a <a href="http://nltk.org/api/nltk.html?highlight=ngrams#nltk.util.ngrams">function</a> for it:
``` python generating ngrams with nltk</p>

<blockquote><blockquote><blockquote><p>from nltk.util import ngrams
generated_ngrams = ngrams('TEXT', 4, pad_left=True, pad_right=True, pad_symbol=' ')
generated_ngrams[4]
('E', 'X', 'T', ' ')
```</p></blockquote></blockquote></blockquote>

<p>ngram function will return a tuple so we need to join positions in ngrams itself:
``` python join splitted char into a ngram</p>

<blockquote><blockquote><blockquote><p>''.join(generated_ngrams[4])
'EXT '
```</p></blockquote></blockquote></blockquote>

<h3>Third step: Hashing into a table and countig each N-gram occurrences</h3>

<p>The easiest way to do this is using a python dictionary, doing a sum when ngram has been seen before or creating a new key otherwise:
``` python counting ngrams occurrences
ngrams_statistics = {}</p>

<p>for ngram in ngrams:</p>

<pre><code>if not ngrams_statistics.has_key(ngram):
    ngrams_statistics.update({ngram:1})
else:
    ngram_occurrences = ngrams_statistics[ngram]
    ngrams_statistics.update({ngram:ngram_occurrences+1})
</code></pre>

<p>```</p>

<h3>Fourth step: Sort in reverse order by number of occurrences</h3>

<p>By last, we need to sort previously created dictionary in reverse order based on each ngram occurrences to keep just top 300 most repeated ngrams. Python dict's can't be sorted, so we need to transform it to a sorted list, we can easily achieve it using operator module:
``` python converting a dictionary to a sorted list</p>

<blockquote><blockquote><blockquote><p>ngrams_statistics_sorted = sorted(ngrams_statistics.iteritems(),\
...                              key=operator.itemgetter(1),\
...                              reverse=True)[0:300]
ngrams_statistics_sorted[80:90]
[('y', 23994), ('  p', 23941), (' p', 23941), ('   p', 23941), ('    p', 23941), ('b', 23809), ('r ', 23783), ('r    ', 23783), ('r   ', 23783), ('r  ', 23783)]
```</p></blockquote></blockquote></blockquote>

<p>Now it only remains to save "ngrams_statistics_sorted" to a file as a "text category profile" or keep just ngrams without occurrences sum when comparing them against others profiles.</p>

<h3>Comparing profiles</h3>

<p>To categorize a text first we need to load pre-computed categories into a list/dict or something similar and, when loaded, walk it and calculate distance with each previously computed profile:
``` python computing ratios for each pre-computed profile</p>

<blockquote><blockquote><blockquote><p>for language, ngrams_statistics in self.<em>languages_statistics.iteritems():
...     language_ngram_statistics = self.</em>calculate_ngram_occurrences(raw_text)
...     distance = self._compare_ngram_frequency_profiles(ngrams_statistics, language_ngram_statistics)
...     languages_ratios.update({language:distance})</p>

<p>languages_ratios
{'spanish': 39960, 'french': 40398, 'english': 40418}</p>

<h1>extracting the one with lowest distance</h1>

<p>min(languages_ratios, key=languages_ratios.get)
'spanish'
```</p></blockquote></blockquote></blockquote>

<h2>Conclusions</h2>

<p>N-Gram-Based text categorization is probably not the "state-of-art" in text categorization - almost ten years old and a bit simple compared with newer ways of categorizing text - but it could be useful in some situations and as a basis to build upon and, what the heck, i learned doing it and had great time, so it totally worth it to me ;)</p>

<p>See you soon!</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Detecting text language with python and NLTK]]></title>
    <link href="http://blog.alejandronolla.com/2013/05/15/detecting-text-language-with-python-and-nltk/"/>
    <updated>2013-05-15T13:40:00+02:00</updated>
    <id>http://blog.alejandronolla.com/2013/05/15/detecting-text-language-with-python-and-nltk</id>
    <content type="html"><![CDATA[<h2>Introduction</h2>

<p>Most of us are used to Internet search engines and social networks capabilities to show only data in certain language, for example, showing only results written in Spanish or English. To achieve that, indexed text must have been analized previously to "guess" the languange and store it together.</p>

<p>There are several ways to do that; probably the most easy to do is a stopwords based approach. The term "stopword" is used in natural language processing to refer words which should be filtered out from text before doing any kind of processing, commonly because this words are little or nothing usefult at all when analyzing text. <!-- more --></p>

<h2>How to do that?</h2>

<p>Ok, so we have a text whose language we want to detect depending on stopwords being used in such text. First step is to "tokenize" - convert given text to a list of "words" or "tokens" - using an approach or another depending on our requeriments: should we keep contractions or, otherwise, should we split them? we need puntuactions or want to split them off? and so on.</p>

<p>In this case we are going to split all punctuations into separate tokens:
``` python nltk "wordpunct_tokenize" tokenizer</p>

<blockquote><blockquote><blockquote><p>from nltk import wordpunct_tokenize
wordpunct_tokenize("That's thirty minutes away. I'll be there in ten.")
['That', "'", 's', 'thirty', 'minutes', 'away', '.', 'I', "'", 'll', 'be', 'there', 'in', 'ten', '.']
```</p></blockquote></blockquote></blockquote>

<p>As shown, the famous quote from Mr. Wolf has been splitted and now we have "clean" words to match against stopwords list.</p>

<p>At this point we need stopwords for several languages and here is when <a href="http://nltk.org/index.html">NLTK</a> comes to handy:
``` python included languages in NLTK</p>

<blockquote><blockquote><blockquote><p>from nltk.corpus import stopwords
stopwords.fileids()
['danish', 'dutch', 'english', 'finnish', 'french', 'german', 'hungarian', 'italian', 'norwegian', 'portuguese', 'russian', 'spanish', 'swedish', 'turkish']</p>

<p>stopwords.words('english')[0:10]
['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'your']
```</p></blockquote></blockquote></blockquote>

<p>Now we need to compute language probability depending on which stopwords are used:
``` python calculate languages ratios</p>

<blockquote><blockquote><blockquote><p>languages_ratios = {}</p>

<p>tokens = wordpunct_tokenize(text)
words = [word.lower() for word in tokens]</p>

<p>for language in stopwords.fileids():
...     stopwords_set = set(stopwords.words(language))
...     words_set = set(words)
...     common_elements = words_set.intersection(stopwords_set)
...
...     languages_ratios[language] = len(common_elements) # language "score"</p>

<p>languages_ratios
{'swedish': 1, 'danish': 1, 'hungarian': 2, 'finnish': 0, 'portuguese': 0, 'german': 1, 'dutch': 1, 'french': 1, 'spanish': 0, 'norwegian': 1, 'english': 6, 'russian': 0, 'turkish': 0, 'italian': 2}
```</p></blockquote></blockquote></blockquote>

<p>First we tokenize using wordpunct_tokenize function and lowercase all splitted tokens, then we walk across nltk included languages and count how many unique stopwords are seen in analyzed text to put this in "language_ratios" dictionary.</p>

<p>Finally, we only have to get the "key" with biggest "value":
``` python get most rated language</p>

<blockquote><blockquote><blockquote><p>most_rated_language = max(languages_ratios, key=languages_ratios.get)
most_rated_language
'english'
```</p></blockquote></blockquote></blockquote>

<p>So yes, it seems this approach works fine with well written texts - those who respect grammatical rules - (and not so small ones) and is really easy to implement.</p>

<h2>Putting it all together</h2>

<p>If we put all the explained above into a script we have something like this:
``` python langdetector.py</p>

<h1>!/usr/bin/env python</h1>

<h1>coding:utf-8</h1>

<h1>Author: Alejandro Nolla - z0mbiehunt3r</h1>

<h1>Purpose: Example for detecting language using a stopwords based approach</h1>

<h1>Created: 15/05/13</h1>

<p>import sys</p>

<p>try:</p>

<pre><code>from nltk import wordpunct_tokenize
from nltk.corpus import stopwords
</code></pre>

<p>except ImportError:</p>

<pre><code>print '[!] You need to install nltk (http://nltk.org/index.html)'
</code></pre>

<h1>----------------------------------------------------------------------</h1>

<p>def _calculate_languages_ratios(text):</p>

<pre><code>"""
Calculate probability of given text to be written in several languages and
return a dictionary that looks like {'french': 2, 'spanish': 4, 'english': 0}

@param text: Text whose language want to be detected
@type text: str

@return: Dictionary with languages and unique stopwords seen in analyzed text
@rtype: dict
"""

languages_ratios = {}

'''
nltk.wordpunct_tokenize() splits all punctuations into separate tokens

&gt;&gt;&gt; wordpunct_tokenize("That's thirty minutes away. I'll be there in ten.")
['That', "'", 's', 'thirty', 'minutes', 'away', '.', 'I', "'", 'll', 'be', 'there', 'in', 'ten', '.']
'''

tokens = wordpunct_tokenize(text)
words = [word.lower() for word in tokens]

# Compute per language included in nltk number of unique stopwords appearing in analyzed text
for language in stopwords.fileids():
    stopwords_set = set(stopwords.words(language))
    words_set = set(words)
    common_elements = words_set.intersection(stopwords_set)

    languages_ratios[language] = len(common_elements) # language "score"

return languages_ratios
</code></pre>

<h1>----------------------------------------------------------------------</h1>

<p>def detect_language(text):</p>

<pre><code>"""
Calculate probability of given text to be written in several languages and
return the highest scored.

It uses a stopwords based approach, counting how many unique stopwords
are seen in analyzed text.

@param text: Text whose language want to be detected
@type text: str

@return: Most scored language guessed
@rtype: str
"""

ratios = _calculate_languages_ratios(text)

most_rated_language = max(ratios, key=ratios.get)

return most_rated_language
</code></pre>

<p>if <strong>name</strong>=='<strong>main</strong>':</p>

<pre><code>text = '''
There's a passage I got memorized. Ezekiel 25:17. "The path of the righteous man is beset on all sides\
by the inequities of the selfish and the tyranny of evil men. Blessed is he who, in the name of charity\
and good will, shepherds the weak through the valley of the darkness, for he is truly his brother's keeper\
and the finder of lost children. And I will strike down upon thee with great vengeance and furious anger\
those who attempt to poison and destroy My brothers. And you will know I am the Lord when I lay My vengeance\
upon you." Now... I been sayin' that shit for years. And if you ever heard it, that meant your ass. You'd\
be dead right now. I never gave much thought to what it meant. I just thought it was a cold-blooded thing\
to say to a motherfucker before I popped a cap in his ass. But I saw some shit this mornin' made me think\
twice. See, now I'm thinking: maybe it means you're the evil man. And I'm the righteous man. And Mr.\
9mm here... he's the shepherd protecting my righteous ass in the valley of darkness. Or it could mean\
you're the righteous man and I'm the shepherd and it's the world that's evil and selfish. And I'd like\
that. But that shit ain't the truth. The truth is you're the weak. And I'm the tyranny of evil men.\
But I'm tryin', Ringo. I'm tryin' real hard to be the shepherd.
'''

language = detect_language(text)

print language
</code></pre>

<p>```</p>

<p>There are others ways to "guess" language from a given text like N-Gram-Based text categorization so will see it in, probably, next post.</p>

<p>See you soon and, as always, hope you find it interesting and useful!</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[solr: Improving queries performance]]></title>
    <link href="http://blog.alejandronolla.com/2013/04/29/solr-improving-queries-performance/"/>
    <updated>2013-04-29T20:29:00+02:00</updated>
    <id>http://blog.alejandronolla.com/2013/04/29/solr-improving-queries-performance</id>
    <content type="html"><![CDATA[<h2>Introduction</h2>

<p>These days i'm messing around with an application that index thousands of documents per day and perform hundreds of queries per hour, so query performance is crucial. The main aim is to provide detection of URLs and IP addresses (want to play a bit? take a look to <a href="http://blog.alejandronolla.com/2013/03/23/indexing-pdf-for-osint-and-pentesting/">a previous post</a>) but full-text searching capabilities is also desired althought less used, so i have given a try to improve performance and, specifically, query times, and here is my tests results.<!-- more --></p>

<p>Actually the core' schema.xml it's something like this:
``` xml initial schema.xml file
&lt;?xml version="1.0" encoding="UTF-8"?>
<schema name="example" version="1.5">
  <fields></p>

<pre><code>&lt;field name="id" type="uuid" indexed="true" stored="true" default="NEW" multiValued="false" /&gt;
&lt;field name="text" type="text_general" indexed="true" stored="true" multiValued="false"/&gt;
&lt;field name="url" type="string" indexed="true" stored="true" multiValued="false" required="true"/&gt;
&lt;field name="hash" type="string" indexed="true" required="true" stored="true" multiValued="false"/&gt;
</code></pre>

<p>  </fields>
  <uniqueKey>hash</uniqueKey>
  <types></p>

<pre><code>&lt;fieldType name="uuid" class="solr.UUIDField" indexed="true" /&gt;
&lt;fieldType name="string" class="solr.StrField" sortMissingLast="true"/&gt;
&lt;fieldType name="text_general" class="solr.TextField"&gt;
  &lt;analyzer type="index"&gt;
    &lt;tokenizer class="solr.WhitespaceTokenizerFactory"/&gt;
    &lt;filter class="solr.LowerCaseFilterFactory"/&gt;
  &lt;/analyzer&gt;
  &lt;analyzer type="query"&gt;
    &lt;tokenizer class="solr.WhitespaceTokenizerFactory"/&gt;
    &lt;filter class="solr.LowerCaseFilterFactory"/&gt;
  &lt;/analyzer&gt;
&lt;/fieldType&gt;
</code></pre>

<p>  </types>
</schema>
```<br/>
As can be seen, it only indexes and store given text, url and hash (used for avoid dupes), converting case to lower and tokenizing by whitespaces. This means that a document with content "SPAM Visit blog.alejandronolla.com" will be tokenized to "['spam', 'visit', 'blog.alejandronolla.com']" so, if we want to search documents mentioning any subdomain of alejandronolla.com we would have to search something like "text:*alejandronolla.com" (it could vary based on decisions like looking for domains similar to alejandronolla.com.PHISINGSITE.com or just whatever.alejandronolla.com).</p>

<p>This kind of queries, using leading/trailing wildcars, are really expensive for solr because it can't use just indexed tokens but perform some walking up to "n" characters more.</p>

<h2>Avoiding solr heap space problems</h2>

<p>When dealing with a lot of documents concurrently probably you're going to face heap space problems sooner or later so i strongly recommend to increase RAM asigned to java virtual machine.</p>

<p>In this case i use Tomcat to serve solr, so i needed to modify JAVA_OPTS in catalina.sh (stored at <em>"/usr/share/tomcat7/bin/catalina.sh"</em>):
<code>bash Setting up values for RAM usage
if [ -z "$LOGGING_MANAGER" ]; then
  JAVA_OPTS="$JAVA_OPTS -Djava.util.logging.manager=org.apache.juli.ClassLoaderLogManager -Xms2048m -Xmx16384m"
else
  JAVA_OPTS="$JAVA_OPTS $LOGGING_MANAGER -Xms2048m -Xmx16384m"
fi
</code></p>

<p>Adding <em>"-Xms2048m -Xmx16384m"</em> we specify tomcat to preallocate at least 2048MB and maximum of 16384MB for heap space for avoiding heap space problems (in my tests i almost used about 2GB indexing about 300k documents in two differents cores, so there is plenty of RAM left yet):<br/>
<img src="/images/upload/2013/04/jvm.png"></p>

<h2>Handling thousand of concurrent connections with Tomcat</h2>

<p>We have to set some configuration at <em>"/etc/tomcat6/server.xml"</em>:<br/>
``` xml Tomcat configuration</p>

<pre><code>&lt;Connector port="8080" protocol="HTTP/1.1" 
           connectionTimeout="20000" 
           URIEncoding="UTF-8"
           redirectPort="8443"
           maxThreads="10000"/&gt;
</code></pre>

<p>```<br/>
I have set up maxThreads to 10000 because i want to index documents through API REST with a python script using async HTTP requests to avoid loosing too much time indexing data (and i'm almost sure bottleneck here is python and not solr).</p>

<h2>First improvement: Separate the grain from the chaff</h2>

<p>As previously said, most of the queries looks for domains and IP addresses through full document's content, causing really heavy queries (and performance problems), so the first action i took was to create a new fields just with "domains look's like" string and IP addresses to tie down queries just to potentially valuable info.</p>

<p>To extract domains, emails and similar strings solr already have a really powerful tokenizer called <a href="http://wiki.apache.org/solr/AnalyzersTokenizersTokenFilters#solr.UAX29URLEmailTokenizerFactory">solr.UAX29URLEmailTokenizerFactory</a>, so we only need to tell solr to index given document text using this tokenizer in another field.</p>

<p>To specify solr which and where field we want to copy we have to create two new fields and specify source and destination fields:<br/>
``` xml New copied fields at schema.xml
  <fields></p>

<pre><code>[...]
&lt;field name="ip_addresses" type="ip_addresses" indexed="true" stored="false" multiValued="false"/&gt;
&lt;field name="text_UAX29" type="text_UAX29" indexed="true" stored="false" multiValued="false"/&gt;
</code></pre>

<p>  </fields></p>

<p>  <copyField source="text" dest="text_UAX29"/>
  <copyField source="text" dest="ip_addresses"/></p>

<p>  <types></p>

<pre><code>[...]
&lt;fieldType name="text_UAX29" class="solr.TextField" positionIncrementGap="100"&gt;
  &lt;analyzer type="index"&gt;
    &lt;tokenizer class="solr.UAX29URLEmailTokenizerFactory"/&gt;
    &lt;filter class="solr.LowerCaseFilterFactory"/&gt;
  &lt;/analyzer&gt;
  &lt;analyzer type="query"&gt;
    &lt;tokenizer class="solr.UAX29URLEmailTokenizerFactory"/&gt;
    &lt;filter class="solr.LowerCaseFilterFactory"/&gt;
  &lt;/analyzer&gt;
&lt;/fieldType&gt;
[...]
&lt;/types&gt;
</code></pre>

<p>```<br/>
We are going to use these fields only for searching, so we specify to index but not store (we already have full document content in "text" field) It's important to have in mind the fact that solr copy fields <a href="http://wiki.apache.org/solr/SchemaXml#Copy_Fields">before</a> doing any kind of processing to document.</p>

<p>If you have noticed it, we specified an undeclared field type called "ip_addresses", and we are going to use <a href="http://wiki.apache.org/solr/AnalyzersTokenizersTokenFilters#solr.PatternTokenizerFactory">solr.PatternTokenizerFactory</a> to make a regex for extracting IP addresses and CIDR network ranges (like 192.168.1.0/16)<br/>
``` xml Extracting IP addresses with regex
  <types></p>

<pre><code>[...]
&lt;fieldType name="ip_addresses" class="solr.TextField"&gt;
  &lt;analyzer&gt;
    &lt;tokenizer class="solr.PatternTokenizerFactory" pattern="(\d{1,3}\.\d{1,3}\.\d{1,3}\.\d{1,3}/?\d{1,2})" group="1" /&gt;
  &lt;/analyzer&gt;
&lt;/fieldType&gt; 
[...]
&lt;/types&gt;
</code></pre>

<p>```<br/>
It's a really simple regex and should be improved before using it in a production environment for example, to extract only certain IP addresses (not RFC1918, not bogus, quad-octet validated, and so on) or even implement your own tokenizer extending <a href="http://wiki.apache.org/solr/AnalyzersTokenizersTokenFilters">existing ones</a>, but will fit ok for our tests.</p>

<p>Now we can change queries from "text:*alejandronolla.com" to "text_UAX29:*alejandronolla.com" to walk much smaller subset of data, improving queries in a huge way.</p>

<h2>Second improvement: Don't waste resources in features not being used</h2>

<p>Solr is a really powerful full-text search engine and, as such, it is able to perform several kind of analysis for indexed data in an automated way. Obviously those analysis need resources to be made so we are wasting CPU cycles and RAM if we are not going to use them.</p>

<p>One of these features is related to solr capability for boosting some query results over others and is based on certain "weight". For example, two documents mentioning "solr" keyword just one time - one with a length of just few words and the other having several thousands - will have different relevances for solr engine, being more important the smallest one. This is because of term frequency-inverse document frequeny (usually refered as <a href="http://en.wikipedia.org/wiki/Tf%E2%80%93idf">tf-idf</a>) statistic approach, if same keyword appear same number of time it represents a bigger percentage of the entire document in the smallest one.</p>

<p>Because we are not going to use this feature we can disable it and save some resources modifying schema.xml file:<br/>
``` xml Avoiding some statistical analysis
  <fields></p>

<pre><code>[...]
&lt;field name="text" type="text_general" indexed="true" stored="true" multiValued="false" omitNorms="true"/&gt;
&lt;field name="url" type="string" indexed="true" stored="true" multiValued="false" required="true" omitNorms="true"/&gt;
[...]
&lt;field name="ip_addresses" type="ip_addresses" indexed="true" stored="false" multiValued="false" omitNorms="true"/&gt;
&lt;field name="text_UAX29" type="text_UAX29" indexed="true" stored="false" multiValued="false" omitNorms="true"/&gt;
[...]
</code></pre>

<p>  </fields>
```<br/>
By setting "omitNorms" to "true" we specify solr to not don't care about length normalization or index-time boosting, you can check the <a href="http://wiki.apache.org/solr/SchemaXml">wiki</a> for more information.</p>

<p>Another feature we don't need now is the solr ability to find similar documents to given one (feature called <a href="http://wiki.apache.org/solr/MoreLikeThis">MoreLikeThis</a>). To do this we can take several approaches as compare tf-idf values or, more accurate way, represent each document as a vector (<a href="http://en.wikipedia.org/wiki/Vector_space_model">vector space model</a>) and find near ones (solr mix both).</p>

<p>Because we are not going to use this feature we can set it off by specifying following field options:
``` xml Disabling vector space model at schema.xml
  <fields></p>

<pre><code>[...]
&lt;field name="text" type="text_general" indexed="true" stored="true" multiValued="false" omitNorms="true" termVectors="false" termsPositions="false" termOffsets="false"/&gt;
&lt;field name="url" type="string" indexed="true" stored="true" multiValued="false" required="true" omitNorms="true" termVectors="false" termsPositions="false" termOffsets="false"/&gt;
[...]
&lt;field name="ip_addresses" type="ip_addresses" indexed="true" stored="false" multiValued="false" omitNorms="true" termVectors="false" termsPositions="false" termOffsets="false"/&gt;
&lt;field name="text_UAX29" type="text_UAX29" indexed="true" stored="false" multiValued="false" omitNorms="true" termVectors="false" termsPositions="false" termOffsets="false"/&gt;
[...]
</code></pre>

<p>  </fields>
```<br/>
I have disabled them with these options "<em>termVectors="false" termsPositions="false" termOffsets="false"</em>" and gain some performance boost.</p>

<p>If you want to know which field options to use based on your application aim take a look to official <a href="http://wiki.apache.org/solr/FieldOptionsByUseCase">wiki</a>:<br/>
<img src="/images/upload/2013/04/fieldoptionsbyusecase.png"></p>

<h2>Third improvement: Avoid indexing stopwords</h2>

<p>When doing natural lenguage processing the term "stopwords" is used to refer those words that should be removed before processing text because of their uselessness. For example, when indexing a document with content like "Visit me at blog.alejandronolla.com" we don't care about personal pronoun "me" and preposition "at" (<a href="http://en.wikipedia.org/wiki/Part-of-speech_tagging">take a look to part-of-speech tagging</a>) so less indexed words, less used resources.</p>

<p>To avoid processing those words we need to specify solr where stopwords are located:<br/>
``` xml Avoiding stopwords being processed
  <fields></p>

<pre><code>[...]
&lt;fieldType name="text_general" class="solr.TextField"&gt;
  &lt;analyzer type="index"&gt;
    &lt;tokenizer class="solr.WhitespaceTokenizerFactory"/&gt;
    &lt;filter class="solr.StopFilterFactory" ignoreCase="true" words="stopwords.txt" enablePositionIncrements="true" /&gt;
    &lt;filter class="solr.LowerCaseFilterFactory"/&gt;
  &lt;/analyzer&gt;
  &lt;analyzer type="query"&gt;
    &lt;tokenizer class="solr.WhitespaceTokenizerFactory"/&gt;
    &lt;filter class="solr.StopFilterFactory" ignoreCase="true" words="stopwords.txt" enablePositionIncrements="true" /&gt;
    &lt;filter class="solr.LowerCaseFilterFactory"/&gt;
  &lt;/analyzer&gt;
&lt;/fieldType&gt;
[...]
</code></pre>

<p>  </fields>
<code>  
We need to have a file called *"stopwords.txt"* at our *"conf"* directory for specified core containing these words and we can find some stopwords for several languages in the example configuration provided with solr package at *"/PATH/TO/SOLR/CORE/conf/lang"*:  
</code> bash Some English stopwords
root@ph0b0s:/opt/solr/solr/conf/lang# tail stopwords_en.txt
their
then
there
these
they
this
to
was
will
with
```<br/>
Of course, we can also include as stop words common words that don't give us any useful information like dog, bread, ROI, APT and so on...</p>

<h2>Fourth impromevent: Word stemming</h2>

<p>Despite of haven't used stemming yet in solr environments it's possible to convert a given word to his morphological root through an <a href="http://wiki.apache.org/solr/AnalyzersTokenizersTokenFilters#Stemming">stemming</a> process:<br/>
``` python Stemming word with python example</p>

<blockquote><blockquote><blockquote><p>from nltk.stemmer.porter import PorterStemmer
PorterStemmer().stem_word('documents')
'document'
```<br/>
Because we "reduce" words to his root probably few of them, per document, will share stem and this will result in a smaller index and more performance booster.</p></blockquote></blockquote></blockquote>

<h2>Fifth improvement: Don't ask the same two times</h2>

<p>Depending on application data and workflow it could be really useful to cache "n" most common queries/filters/documents and avoid doing over and over the same query in a few minutes apart, i'm sorry but haven't played around too much with it, so to read more about this go to the <a href="http://wiki.apache.org/solr/SolrCaching">wiki</a>.</p>

<h2>Results</h2>

<p>After taking first two improvements actions did some performance test and comparisons, so here are some info for a "small" subset of about 300k documents:</p>

<table border="1">
    <tr>
        <td></td>
        <td align="right">Original schema</td>
        <td align="right">Modified schema</td>
    </tr>
    <tr>
        <td align="left">Indexing time: </td>
        <td align="right">95 minutes</td>
        <td align="right">101 minutes</td>
    </tr>
    <tr>
        <td align="left">Index size: </td>
        <td align="right">555.12 MB</td>
        <td align="right">789.8 MB</td>
    </tr>
    <tr>
        <td align="left">Field being queried: </td>
        <td align="right">text</td>
        <td align="right">text_UAX29</td>
    </tr>
    <tr>
        <td align="left">Worst query scenario: </td>
        <td align="right">84766 milliseconds</td>
        <td align="right">52417 milliseconds</td>
    </tr>
    <tr>
        <td align="left">Worst query improvement: </td>
        <td align="center">--</td>
        <td align="right">38,2% faster</td>
    </tr>
</table>


<p></p>

<p>As shown in the above table, the "worst" query i'm now performing (dozens of logical operators and wildcards) will take about 38% time less per query hit and, in an application which performs hundreds of query per hour, it's a great improvement without disrupting normal functioning (looking for domains and IP addresses) and, in the other hand, it will take almost no more time to index and more than reasonable index size increment that worth it.</p>

<p>Hope you liked it and can apply someway to your needs, see you soon!</p>
]]></content>
  </entry>
  
</feed>
